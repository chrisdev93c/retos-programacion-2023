{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMralsKRt6Vh8b8X+qOscoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisdev93c/retos-programacion-2023/blob/main/scrapertwitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTa3rXunHpi2"
      },
      "outputs": [],
      "source": [
        "!pip install snscrape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "scrapear tweets\n"
      ],
      "metadata": {
        "id": "hJOBVVsTuScM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Máximo número de tweets a extraer\n",
        "max_tweets = 10\n",
        "\n",
        "# Patrones regex para detectar enlaces y hashtags\n",
        "pattern_links = r\"https?://[^\\s]+\"\n",
        "pattern_hashtags = r\"#\\S+\"\n",
        "pattern_emojis = r\"([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])\"\n",
        "pattern_kingsleague = r\"\\bKingsLeague\\b\"\n",
        "pattern_mentions = r\"@\\S+\"\n",
        "pattern_special_chars = r\"[^\\w\\s]\"\n",
        "\n",
        "\n",
        "\n",
        "# Crea una lista vacía para almacenar los tweets\n",
        "tweets = []\n",
        "\n",
        "# Realiza la búsqueda de tweets con el hashtag \"KingsLeague\" en un rango de fechas\n",
        "for tweet in sntwitter.TwitterSearchScraper(\n",
        "    '#kingsleague lang:es since:2022-12-27 until:2023-01-01'\n",
        ").get_items():\n",
        "    # Si se ha alcanzado el límite máximo de tweets, detiene el bucle\n",
        "    if len(tweets) > max_tweets:\n",
        "        break\n",
        "    \n",
        "    # Limpia el texto del tweet eliminando enlaces, hashtags y menciones\n",
        "    text = tweet.content\n",
        "    text = re.sub(pattern_links, \"\", text)\n",
        "    text = re.sub(pattern_hashtags, \"\", text)\n",
        "    text = re.sub(r\"@\\S+\", \"\", text)\n",
        "    text = re.sub(pattern_emojis, \"\", text)\n",
        "    text = re.sub(pattern_kingsleague, \"\", text)\n",
        "    text = re.sub(pattern_mentions, \"\", text)\n",
        "    text = re.sub(pattern_special_chars, \"\", text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Añade el tweet a la lista\n",
        "    tweets.append({\n",
        "        \"text\": text,\n",
        "      \n",
        "        \n",
        "    })\n",
        "\n",
        "# Convierte la lista de tweets en un dataframe de Pandas\n",
        "data = pd.DataFrame(tweets)\n",
        "data.to_csv(\"tweetss.csv\", index=False)\n",
        "\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "fvl0QYrKP6c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "limpiamos el texto para las palabras mas repetidas"
      ],
      "metadata": {
        "id": "1mApLokcuuhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def clean_cell(cell):\n",
        "    # Elimina artículos y preposiciones\n",
        "    cell = re.sub(r\"\\b(El|la|los|las|un|una|unos|unas|del|al)\\b\", \"\", cell)\n",
        "    # Elimina espacios en blanco adicionales\n",
        "    cell = re.sub(r\"\\s+\", \" \", cell).strip()\n",
        "    return cell\n",
        "\n",
        "# Abre el archivo CSV original y el archivo CSV de destino\n",
        "with open(\"tweets.csv\", \"r\") as fin, open(\"tweets_clean.csv\", \"w\") as fout:\n",
        "    # Crea un objeto CSV reader y un objeto CSV writer\n",
        "    reader = csv.reader(fin)\n",
        "    writer = csv.writer(fout)\n",
        "    # Recorre las filas del archivo CSV original\n",
        "    for row in reader:\n",
        "        # Limpia cada celda de la fila\n",
        "        row = [clean_cell(cell) for cell in row]\n",
        "        # Escribe la fila limpia en el archivo CSV de destino\n",
        "        writer.writerow(row)\n"
      ],
      "metadata": {
        "id": "JbAyfaYAtz_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"tweets.csv\")\n",
        "texts = data[\"text\"].tolist()\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "# Separa el texto en una lista de palabras o frases\n",
        "words = [text.split() for text in texts]\n",
        "\n",
        "# Cuenta la frecuencia de cada palabra o frase\n",
        "counter = Counter([word for text in words for word in text])\n",
        "\n",
        "# Ordena las palabras o frases por frecuencia\n",
        "most_common = counter.most_common()\n",
        "\n",
        "# Crea un dataframe de Pandas con las palabras o frases más comunes\n",
        "df = pd.DataFrame(most_common, columns=[\"palabra_o_frase\", \"frecuencia\"])\n",
        "\n",
        "# Guarda el dataframe en un archivo CSV\n",
        "df.to_csv(\"palabras_o_frases_mas_comune.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "2K_3GdY2dVf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "id": "4uJTZoj6i2OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textblob\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Abre el archivo CSV y carga los datos en un dataframe\n",
        "data = pd.read_csv(\"tweets.csv\")\n",
        "\n",
        "# Itera sobre las filas del archivo CSV\n",
        "for index, row in data.iterrows():\n",
        "    # Traduce el texto al inglés\n",
        "    #translation = TextBlob(row[\"text\"]).translate(from_lang='es', to='en')\n",
        "    #print(translation)\n",
        "    text = TextBlob(row[\"text\"]).translate(from_lang='es', to='en').raw\n",
        "\n",
        "    # Obtiene el texto traducido\n",
        "    #text = translation.text\n",
        "    # Crea un objeto TextBlob a partir del texto traducido\n",
        "    blob = TextBlob(text)\n",
        "    # Obtiene el análisis de sentimientos\n",
        "    sentiment = blob.sentiment\n",
        "    # Clasifica el sentimiento como negativo, positivo o neutro\n",
        "    if sentiment.polarity < 0:\n",
        "        sentimiento = \"negativo\"\n",
        "    elif sentiment.polarity > 0:\n",
        "        sentimiento = \"positivo\"\n",
        "    else:\n",
        "        sentimiento = \"neutro\"\n",
        "    # Añade el sentimiento al dataframe\n",
        "    data.at[index, \"sentimiento\"] = sentimiento\n",
        "  \n",
        "    data.to_csv(\"tweets.csv\", index=False)"
      ],
      "metadata": {
        "id": "RFnfSP1VkmqS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}